{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* The objective of this assignments is to build the **Decoder** part of the Transformer architecture.\n",
        "* We will be using the **PyTorch** framework to implement the following components\n",
        "  * Decoder Layer that contains\n",
        "    * Multi-Head Masked Attention (MHMA) Module\n",
        "    * Multi-Head Cross Attention (MHMA) Module\n",
        "    * Position-wise Feed Forward Neural Network\n",
        "\n",
        "  * Implement CLM\n",
        "\n",
        "* **DO NOT** USE Built-in **TRANSFORMER LAYERS** as it affects the reproducibility.\n",
        "\n",
        "* You will be given with a configuration file that contains information on various hyperparameters such as embedding dimension, vocabulary size,number heads and so on\n",
        "\n",
        "* Use ReLU activation function and Stochastic Gradient Descent optimizer\n",
        "* Here are a list of helpful Pytorch functions (does not mean you have to use all of them) for this subsequent assignments\n",
        "  * [torch.matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch-matmul)\n",
        "  * [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "  * torch.swapdims\n",
        "  * torch.unsqueeze\n",
        "  * torch.squeeze\n",
        "  * torch.argmax\n",
        "  * [torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)\n",
        "  * [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "  * [torch.nn.Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)\n",
        "  * torch.nn.Linear\n",
        "  * torch.nn.LayerNorm\n",
        "  * torch.nn.ModuleList\n",
        "  * torch.nn.Sequential\n",
        "  * [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  \n",
        "* Important: Do not set any global seeds.\n",
        "\n",
        "* Helpful resources to get started with\n",
        "\n",
        " * [Andrej Karpathys Nano GPT](https://github.com/karpathy/nanoGPT)\n",
        " * [PyTorch Source code of Transformer Layer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "SLKgMhMp8wvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es7WIeXF3nrV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import requests\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#do not edit this cell\n",
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/dec_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "id": "de2_k13-3_gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = config['input']['vocab_size']\n",
        "batch_size = config['input']['batch_size']\n",
        "seq_len = config['input']['seq_len']\n",
        "embed_dim = config['input']['embed_dim']\n",
        "dmodel = embed_dim\n",
        "dq = torch.tensor(config['model']['dq'])\n",
        "dk = torch.tensor(config['model']['dk'])\n",
        "dv = torch.tensor(config['model']['dv'])\n",
        "heads = torch.tensor(config['model']['n_heads'])\n",
        "d_ff = config['model']['d_ff']"
      ],
      "metadata": {
        "id": "kS7dciwb53M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input tokens"
      ],
      "metadata": {
        "id": "6uLkQpFA5-Gs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Generate a raw_input ids (without any special tokens appended to it)\n",
        "\n",
        "* Since we will be using this as label after adding the special  \\<start\\> token, we use the variable name \"label_ids\"\n",
        "\n",
        "* Keep the size of the `label_ids=(bs,seq_len-1)` as we insert a special token ids in the next step"
      ],
      "metadata": {
        "id": "PfUUsznGp9Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "data_url = 'https://github.com/Arunprakash-A/LLM-from-scratch-PyTorch/raw/main/config_files/w2_input_tokens'\n",
        "r = requests.get(data_url)\n",
        "label_ids = torch.load(BytesIO(r.content))"
      ],
      "metadata": {
        "id": "I_D8hvdH5_1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let the first token_id be be a special `[start]` token (mapped to integer 0)\n",
        "* If label_ids=$\\begin{bmatrix}1&2\\\\3&4 \\end{bmatrix}$, then we modify it as $\\begin{bmatrix}0&1&2\\\\0&3&4 \\end{bmatrix}$"
      ],
      "metadata": {
        "id": "nxmtUf5j6JJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = None # the first column of token_ids should be zeros and the rest of the columns come from label_ids"
      ],
      "metadata": {
        "id": "ADbTWVVC6fu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement the following components of a decoder layer\n",
        "\n",
        " * Multi-head Masked Attention (MHMA)\n",
        " * Multi-head Cross Attention (MHCA)\n",
        " * Postion-wise FFN\n",
        "\n",
        "* Randomly initialize the parameters using normal distribution with the following seed values\n",
        "  * $W_Q:$(seed=43)\n",
        "  * $W_K:$(seed=44)\n",
        "  * $W_V:$(seed=45)\n",
        "  * $W_O:$(seed=46)\n",
        "\n",
        "* Remember that, Multi-head cross atention takes two represnetation. One is the encoder output and the other one is the output from masked attetnion sub-layer.\n",
        "\n",
        "* However, in this assignment, we will fix it to a random matrix."
      ],
      "metadata": {
        "id": "nrDQbLIF334F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHCA(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,heads):\n",
        "    super(MHCA,self).__init__()\n",
        "\n",
        "\n",
        "     # your method definitions go here (if you want to)\n",
        "\n",
        "  def forward(self):\n",
        "\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "2URLKYHT8i1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By default, `mask=None`. Therefore, create and apply the mask while computing the attention scores\n"
      ],
      "metadata": {
        "id": "3axiToa8AiTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHMA(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,heads,mask=None):\n",
        "    super(MHMA,self).__init__()\n",
        "    # your code goes here\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self):\n",
        "    # implement forward method\n",
        "    return out"
      ],
      "metadata": {
        "id": "fkRKwCzY_uOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the FFN and OutputLayer modules (same as the one you implemented for encoder)"
      ],
      "metadata": {
        "id": "HkWY1xuzbV1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,d_ff):\n",
        "    super(FFN,self).__init__()\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "WriFqQWHAGZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,vocab_size):\n",
        "    super(OutputLayer,self).__init__()\n",
        "\n",
        "\n",
        "  def forward(self,):\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "sZ8mE5xQbzh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implement the final decoder layer."
      ],
      "metadata": {
        "id": "htg7Hx-Kb1_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self,dmodel,dq,dk,dv,d_ff,heads,mask=None):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.mhma = MHMA(dmodel,dq,dk,dv,heads,mask=None)\n",
        "    self.mhca = MHCA(dmodel,dq,dk,dv,heads)\n",
        "    self.layer_norm_mhma = torch.nn.LayerNorm(dmodel)\n",
        "    self.layer_norm_mhca = torch.nn.LayerNorm(dmodel)\n",
        "    self.layer_norm_ffn = torch.nn.LayerNorm(dmodel)\n",
        "    self.ffn = FFN(dmodel,d_ff)\n",
        "\n",
        "  def forward(self,):\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "y3KrOhIj_1YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create an embedding layer that takes in token_ids and return embeddings for the token_ids\n",
        "\n",
        " * Use seed value: 70"
      ],
      "metadata": {
        "id": "0on1cUNEcEt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,embed_dim):\n",
        "    super(Embed,self).__init__()\n",
        "\n",
        "    self.embed= None\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.embed(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "0a--wC-_Tf_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "\n",
        " * Implement the decoder that has `num_layers` decoder layers"
      ],
      "metadata": {
        "id": "Xr3nTQP5d2zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask,num_layers=1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embed_lookup = None\n",
        "    self.dec_layers = None\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self,enc_rep,tar_token_ids):\n",
        "\n",
        "\n",
        "    return out=None"
      ],
      "metadata": {
        "id": "SYRrxMkpBzAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Representation from encoder\n",
        "\n",
        " * Since all the decoder layers require the representation from the encoder to compute cross attention, we are going to feed in the random values (Note, it does not require gradient during training)"
      ],
      "metadata": {
        "id": "lyZYR7NkCYh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this\n",
        "enc_rep = torch.randn(size=(batch_size,seq_len,embed_dim),generator=torch.random.manual_seed(10))"
      ],
      "metadata": {
        "id": "-rOysYMjB-af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the model"
      ],
      "metadata": {
        "id": "tpLKJVXnGbbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Decoder(vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask=None)"
      ],
      "metadata": {
        "id": "aeG9CjI_GcsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "aebVyW-9LhcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(enc_rep,tar_token_ids,label_ids,epochs=1000):\n",
        "  loss_trace = []\n",
        "  for epoch in range(epochs):\n",
        "    out = model(enc_rep,tar_token_ids)\n",
        "    loss =\n",
        "    loss.backward()\n",
        "\n",
        "    #update parameters\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "8npNWRJ7tWRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train the model for 1000 epochs"
      ],
      "metadata": {
        "id": "lXLdvUVKgU3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(enc_rep,token_ids,label_ids,1000)"
      ],
      "metadata": {
        "id": "svrd7-yHtpme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  predictions = torch.argmax(model(enc_rep,token_ids),dim=-1)"
      ],
      "metadata": {
        "id": "M6PLInPW0JIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The loss will be around 0.17 after 1000 epochs"
      ],
      "metadata": {
        "id": "TsA-osXehjEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# number of correct predictions\n",
        "print(torch.count_nonzero(label_ids==predictions[:,0:-1]))"
      ],
      "metadata": {
        "id": "hbnfyb5thxCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* THe number of correct predictions is close to 66"
      ],
      "metadata": {
        "id": "UXTPr2pd6tfo"
      }
    }
  ]
}
